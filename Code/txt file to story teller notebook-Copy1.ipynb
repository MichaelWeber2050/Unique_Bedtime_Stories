{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependecies on these packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import keras.models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the txt data and preview \n",
    "dirty_text=(open('your_file_name').read())\n",
    "dirty_text=dirty_text.lower()\n",
    "dirty_text[0:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disgusting cleaning one by one \n",
    "\n",
    "# need to re write \n",
    "# remove double spaces and line endings to avoid counting\n",
    "\n",
    "dirty_text = dirty_text.replace(\"'\", \"\")\n",
    "dirty_text = dirty_text.replace(\"-\", \"\")\n",
    "dirty_text = dirty_text.replace(\":\", \"\")\n",
    "dirty_text = dirty_text.replace(\"2\", \"\")\n",
    "dirty_text = dirty_text.replace(\"0\", \"\")\n",
    "dirty_text = dirty_text.replace(\"3\", \"\")\n",
    "dirty_text = dirty_text.replace(\"[\", \"\")\n",
    "dirty_text = dirty_text.replace(\"#\", \"\")\n",
    "dirty_text = dirty_text.replace(\"1\", \"\")\n",
    "dirty_text = dirty_text.replace(\"5\", \"\")\n",
    "dirty_text = dirty_text.replace(\"]\", \"\")\n",
    "dirty_text = dirty_text.replace(\"6\", \"\")\n",
    "dirty_text = dirty_text.replace(\"8\", \"\")\n",
    "dirty_text = dirty_text.replace(\"*\", \"\")\n",
    "dirty_text = dirty_text.replace(\"’\", \"\")\n",
    "dirty_text = dirty_text.replace(\"9\", \"\")\n",
    "dirty_text = dirty_text.replace(\"4\", \"\")\n",
    "dirty_text = dirty_text.replace(\"—\", \"\")\n",
    "dirty_text = dirty_text.replace(\"_\", \"\")\n",
    "dirty_text = dirty_text.replace(\"ü\", \"u\")\n",
    "dirty_text = dirty_text.replace(\"(\", \"\")\n",
    "dirty_text = dirty_text.replace(\"î\", \"i\")\n",
    "dirty_text = dirty_text.replace(\"ô\", \"o\")\n",
    "dirty_text = dirty_text.replace(\")\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "dirty_text = dirty_text.replace(\"\\n\", \" \")\n",
    "dirty_text = dirty_text.replace(\"  \", \" \")\n",
    "\n",
    "dirty_text = dirty_text.replace(\" ,  \", \", \")\n",
    "dirty_text = dirty_text.replace(\"www.gutenberg.org.\", \"\")\n",
    "dirty_text = dirty_text.replace(\".\", \"\")\n",
    "dirty_text = dirty_text.replace(\",\", \"\")\n",
    "dirty_text = dirty_text.replace(\"!\", \"\")\n",
    "dirty_text = dirty_text.replace('\"', \"\")\n",
    "\n",
    "\n",
    "dirty_text = dirty_text.replace(\"  \", \" \")\n",
    "dirty_text = dirty_text.replace(\"“\", \"\")\n",
    "\n",
    "dirty_text = dirty_text.replace(\"”\", \"\")\n",
    "dirty_text = dirty_text.replace(\";\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look back at it\n",
    "clean_text = dirty_text\n",
    "clean_text[0:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the unique characters that appear\n",
    "_characters = sorted(list(set(clean_text)))\n",
    "# map the unique characters to a dictionary with char as key and len of set list as value\n",
    "_n_to_char = {n:char for n, char in enumerate(_characters)}\n",
    "# map the unique characters to a dictionary with len of set list as key and char as value\n",
    "_char_to_n = {char:n for n, char in enumerate(_characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of characters\n",
    "_X = []\n",
    "_Y = []\n",
    "length = len(clean_text)\n",
    "seq_length = 100\n",
    "for i in range(0, length-seq_length, 1):\n",
    "    sequence = clean_text[i:i + seq_length]\n",
    "    label = clean_text[i + seq_length]\n",
    "    _X.append([_char_to_n[char] for char in sequence])\n",
    "    _Y.append(_char_to_n[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_modified = np.reshape(_X, (len(_X), seq_length, 1))\n",
    "# normalize the X data\n",
    "_X_modified = _X_modified / float(len(_characters))\n",
    "# one hot encode the output Y variable \n",
    "_Y_modified = np_utils.to_categorical(_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_ = Sequential()\n",
    "_model_.add(LSTM(700, input_shape=(_X_modified.shape[1], _X_modified.shape[2]), \n",
    "               return_sequences=True))\n",
    "_model_.add(Dropout(0.2))\n",
    "_model_.add(LSTM(700))\n",
    "_model_.add(Dropout(0.2))\n",
    "_model_.add(Dense(_Y_modified.shape[1], activation='softmax'))\n",
    "_model_.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint, do this before fitting but not needed unless you intend \n",
    "# to fit the model below\n",
    "\n",
    "filepath=\"model-weights-{epoch:02d}-{loss:.4f}.file\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                              save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggest using a GPU to fit this model\n",
    "_model_.fit(_X_modified, _Y_modified, \n",
    "            epochs=17, \n",
    "            batch_size=100, \n",
    "            callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"model-weights-EPOCH-LOSS.file\"\n",
    "_model_.load_weights(filename)\n",
    "_model_.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(_characters)\n",
    "\n",
    "start = np.random.randint(0, len(_X)-1)\n",
    "pattern = _X[start]\n",
    "#print(\"Seed:\")\n",
    "#print(\"\\\"\", ''.join([_n_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "\n",
    "# write some words hopefully they make sense \n",
    "for i in range(200):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = dracula_model_1.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = dracula_n_to_char[index]\n",
    "    seq_in = [dracula_n_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\n:)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
